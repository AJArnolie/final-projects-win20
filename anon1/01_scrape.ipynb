{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "OUTPUT_DIR = 'output/'\n",
    "\n",
    "\n",
    "class Company:\n",
    "\n",
    "    def get_url(self, url):\n",
    "        \"\"\"For a given job posting URL, returns the beautiful soup object.\n",
    "        Arguments:\n",
    "        url -- URL to query\n",
    "        \"\"\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    def query(self):\n",
    "        self.scrape_data()\n",
    "        self.clean_data()\n",
    "        self.tabulate_data()\n",
    "\n",
    "    def scrape_data(self):\n",
    "        pass\n",
    "\n",
    "    def clean_data(self):\n",
    "        pass\n",
    "\n",
    "    def tabulate_data(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Microsoft(Company):\n",
    "\n",
    "    def clean_soup(self, soup):\n",
    "        \"\"\"Takes beautiful soup object, removes excess strings, and returns json object.\n",
    "        Arguments:\n",
    "        soup -- beautiful soup object\n",
    "        \"\"\"\n",
    "        soup_str = soup.script.string\n",
    "        start_str = ' /*<!--*/ var phApp = phApp || {\"widgetApiEndpoint\":\"https://careers.microsoft.com/professionals/widgets\",\"country\":\"us\",\"deviceType\":\"desktop\",\"locale\":\"en_us\",\"absUrl\":true,\"refNum\":\"MICRUS\",\"cdnUrl\":\"https://prodcmscdn.azureedge.net/careerconnectresources/p\",\"baseUrl\":\"https://careers.microsoft.com/professionals/us/en/\",\"baseDomain\":\"https://careers.microsoft.com/professionals\",\"phenomTrackURL\":\"careers.microsoft.com/professionals/us/en/phenomtrack.min.js\",\"pageName\":\"search-results\",\"siteType\":\"professionals\",\"rootDomain\":\"https://careers.microsoft.com\",\"pageId\":\"page906\"}; phApp.ddo = '\n",
    "        sep_str = '; phApp.sessionParams = '\n",
    "        soup_str = soup_str.replace(start_str,'',1)\n",
    "        soup_str = soup_str.split(sep_str, 1)[0] # remove everything after sep_str\n",
    "        json_file = json.loads(soup_str) # convert to json\n",
    "        return json_file\n",
    "\n",
    "    def get_json(self, url):\n",
    "        \"\"\"Takes url and returns json object.\n",
    "        Arguments:\n",
    "        url -- url string\n",
    "        \"\"\"\n",
    "        soup = self.get_url(url)\n",
    "        json = self.clean_soup(soup)\n",
    "        return json\n",
    "\n",
    "    def get_dataframe(self, num):\n",
    "        \"\"\"Takes list of jobs for a given page number and inserts it into a dataframe. Returns a dataframe.\n",
    "        Arguments:\n",
    "        num -- page number\n",
    "        \"\"\"\n",
    "        num_str = str(num)\n",
    "        url = 'https://careers.microsoft.com/professionals/us/en/search-results?keywords=engineer&from=' + num_str + '&s=1'\n",
    "        json_file = self.get_json(url)\n",
    "        jobs = json_file['eagerLoadRefineSearch']['data']['jobs'] # extract jobs\n",
    "        df = pd.DataFrame(jobs) # put in dataframe\n",
    "        return df\n",
    "\n",
    "    def get_num_jobs(self):\n",
    "        \"\"\"Computes and returns the total number of jobs found.\"\"\"\n",
    "        url = 'https://careers.microsoft.com/professionals/us/en/search-results?keywords=engineer'\n",
    "        json_file = self.get_json(url)\n",
    "        num_jobs = json_file['eagerLoadRefineSearch']['totalHits'] # extract total number of jobs\n",
    "        return num_jobs\n",
    "\n",
    "    def scrape_data(self):\n",
    "        \"\"\"Scrapes data and outputs to csv file. \"\"\"\n",
    "        print(\"Scraping Microsoft...\")\n",
    "        microsoft = self.get_dataframe(0)\n",
    "        num_jobs = self.get_num_jobs() # get total number of results\n",
    "        for i in range(50,num_jobs,50):\n",
    "            microsoft = pd.concat([self.get_dataframe(i), microsoft])\n",
    "        microsoft.to_csv(OUTPUT_DIR+'microsoft.csv')\n",
    "        print(\"Done scraping.\")\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Reads in csv, cleans data, and outputs to csv file.\"\"\"\n",
    "        print(\"Extracting US Engineering jobs...\")\n",
    "        microsoft = pd.read_csv(OUTPUT_DIR+'microsoft.csv')\n",
    "        microsoft = microsoft[microsoft['title'].str.lower().str.contains('engineer')] # get only titles with \"engineer\"\n",
    "        microsoft = microsoft[microsoft['country']=='United States'] # get only jobs in US\n",
    "        microsoft.to_csv(OUTPUT_DIR+'microsoft_US_engineer.csv')\n",
    "\n",
    "    def tabulate_data(self):\n",
    "        \"\"\"Tabulates data by reading in csv file and outputting tabbed csv file.\"\"\"\n",
    "        print(\"Tabulating number of open job titles by city for Microsoft...\")\n",
    "        df_microsoft = pd.read_csv(OUTPUT_DIR+'microsoft_US_engineer.csv')\n",
    "        df_microsoft = df_microsoft.reset_index()\n",
    "        df_microsoft = df_microsoft.drop(['index'], axis=1)\n",
    "        location_lst = []\n",
    "        for i in range(len(df_microsoft)):\n",
    "            loc_lst = ast.literal_eval(df_microsoft.multi_location_array[i])\n",
    "            for loc in loc_lst:\n",
    "                location_lst.append(loc['location'])\n",
    "        locs = df_microsoft['location'].tolist()\n",
    "        all_locations = pd.Series(location_lst + locs)\n",
    "        loc_tabbed = all_locations.astype('str').value_counts()\n",
    "        df_microsoft_tabbed = pd.DataFrame(loc_tabbed)\n",
    "        df_microsoft_tabbed.rename(columns={0: \"num_job_openings\"}, inplace = True)\n",
    "        df_microsoft_tabbed.to_csv(OUTPUT_DIR+'microsoft_tabbed.csv')\n",
    "        print(\"Here are the number of open job titles by city:\")\n",
    "        print(df_microsoft_tabbed)\n",
    "\n",
    "\n",
    "class Jpmorgan(Company):\n",
    "\n",
    "    def get_tags1(self, html_class, soup):\n",
    "        \"\"\"Extracts the html_class from the soup object and does minor cleaning. Cleans version 1.\n",
    "        Arguments:\n",
    "        html_class -- class tag from html\n",
    "        soup -- beautiful soup object\n",
    "        \"\"\"\n",
    "        dest_list = soup.find_all(class_=html_class) \n",
    "        dest = []\n",
    "        for item in dest_list:\n",
    "            dest.append(item.get_text().replace('\\n',''))\n",
    "        return dest\n",
    "\n",
    "    def get_tags2(self, html_class, soup):\n",
    "        \"\"\"Extracts the html_class from the soup object and does minor cleaning. Cleans version 2.\n",
    "        Arguments:\n",
    "        html_class -- class tag from html\n",
    "        soup -- beautiful soup object\n",
    "        \"\"\"\n",
    "        dest_list = soup.find_all(class_=html_class)\n",
    "        dest = []\n",
    "        for item in dest_list[1:]:\n",
    "            dest.append(item.get_text().replace('\\r\\n','').replace('                    ',''))\n",
    "        return dest\n",
    "    \n",
    "    def get_dataframe(self, num):\n",
    "        \"\"\"Takes list of jobs for a given page number and inserts it into a dataframe. Returns a dataframe.\n",
    "        Arguments:\n",
    "        num -- page number\n",
    "        \"\"\"\n",
    "        pagenum = str(num)\n",
    "        url = 'https://jobs.jpmorganchase.com/ListJobs/ByKeyword/engineer/Page-' + pagenum\n",
    "        soup = self.get_url(url)\n",
    "        df_jpmorgan = pd.DataFrame()\n",
    "        df_jpmorgan['job_title'] = self.get_tags1(html_class='coloriginaljobtitle', soup=soup)\n",
    "        df_jpmorgan['jobid'] = self.get_tags1(html_class='coldisplayjobid', soup=soup)[1:]\n",
    "        df_jpmorgan['location_city'] = self.get_tags2(html_class='colcity', soup=soup)\n",
    "        df_jpmorgan['location_state'] = self.get_tags2(html_class='colstate', soup=soup)\n",
    "        df_jpmorgan['location_country'] = self.get_tags2(html_class='colcountry', soup=soup)\n",
    "        df_jpmorgan['date_posted'] = self.get_tags2(html_class='colpostedon', soup=soup)\n",
    "        return df_jpmorgan\n",
    "\n",
    "    def get_num_pages(self):\n",
    "        url = 'https://jobs.jpmorganchase.com/ListJobs/ByKeyword/engineer/'\n",
    "        soup = self.get_url(url)\n",
    "        total_results = soup.find(class_='pager_counts')\n",
    "        num_results = int(total_results.contents[0].split(' of ')[1])\n",
    "        total_num_pages = math.ceil(num_results/30)\n",
    "        return total_num_pages\n",
    "\n",
    "    def scrape_data(self):\n",
    "        \"\"\"Scrapes data and outputs to csv file. Returns dataframe. \"\"\"\n",
    "        print(\"Scraping JPMorgan...\")\n",
    "        jpmorgan = self.get_dataframe(1)\n",
    "        num_pages = self.get_num_pages() # get total number of results\n",
    "        for i in range(2,num_pages+1):\n",
    "            jpmorgan = pd.concat([self.get_dataframe(i), jpmorgan])\n",
    "        jpmorgan.to_csv(OUTPUT_DIR+'jpmorgan.csv')\n",
    "        print(\"Done scraping.\")\n",
    "        return jpmorgan\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Reads in csv, cleans data, and outputs to csv file.\"\"\"\n",
    "        print(\"Extracting JP Morgan US Engineering jobs...\")\n",
    "        df_jpmorgan = pd.read_csv(OUTPUT_DIR+'jpmorgan.csv')\n",
    "        df_jpmorgan = df_jpmorgan[df_jpmorgan['job_title'].str.lower().str.contains('engineer')] # get only titles with \"engineer\"\n",
    "        df_jpmorgan = df_jpmorgan[df_jpmorgan['location_country'] == 'US'] # get only US\n",
    "        df_jpmorgan = df_jpmorgan.reset_index()\n",
    "        df_jpmorgan = df_jpmorgan.drop(['index', 'Unnamed: 0'], axis=1)\n",
    "        df_jpmorgan.to_csv(OUTPUT_DIR+'jpmorgan_US_engineer.csv')\n",
    "\n",
    "    def tabulate_data(self):\n",
    "        \"\"\"Tabulates data by reading in csv file and outputting tabbed csv file.\"\"\"\n",
    "        print(\"Tabulating number of open job titles by city for JP Morgan...\")\n",
    "        df_jpmorgan = pd.read_csv(OUTPUT_DIR+'jpmorgan_US_engineer.csv')\n",
    "        loc_tabbed = df_jpmorgan.groupby([\"location_city\", \"location_state\"]).size()\n",
    "        df_jpmorgan_tabbed = pd.DataFrame(loc_tabbed)\n",
    "        df_jpmorgan_tabbed.rename(columns={0: \"num_job_openings\"}, inplace = True)\n",
    "        df_jpmorgan_tabbed.to_csv(OUTPUT_DIR+'jpmorgan_tabbed.csv')\n",
    "        print(\"Here are the number of open job titles by city:\")\n",
    "        print(df_jpmorgan_tabbed)\n",
    "\n",
    "\n",
    "class Google(Company):\n",
    "\n",
    "    def get_url(self, url):\n",
    "        \"\"\"For a given job posting URL, returns the response object.\n",
    "        Arguments:\n",
    "        url -- URL to query\n",
    "        \"\"\"\n",
    "        response = requests.get(url)\n",
    "        return response\n",
    "\n",
    "    def get_dataframe(self, num):\n",
    "        \"\"\"Takes list of Google jobs for a given page number and inserts it into a dataframe. Returns a dataframe.\n",
    "        Arguments:\n",
    "        num -- page number\n",
    "        \"\"\"\n",
    "        num_str = str(num)\n",
    "        # print(num_str)\n",
    "        url = '''https://careers.google.com/api/jobs/jobs-v1/search/?company=Google&company=Google%20Fiber&company=YouTube&\n",
    "                employment_type=FULL_TIME&hl=en_US&jlo=en_US&location=United%20States&page=''' + num_str + '&q=engineer&sort_by=relevance'\n",
    "        response = self.get_url(url)\n",
    "        jobs_json = response.json()['jobs']\n",
    "        df = pd.DataFrame(jobs_json)\n",
    "        return df\n",
    "\n",
    "    def get_num_jobs(self):\n",
    "        \"\"\"Computes and returns the total number of jobs found.\"\"\"\n",
    "        url = '''https://careers.google.com/api/jobs/jobs-v1/search/?company=Google&company=Google%20Fiber&company=YouTube&\n",
    "                employment_type=FULL_TIME&hl=en_US&jlo=en_US&location=United%20States&page=1&q=engineer&sort_by=relevance'''\n",
    "        response = self.get_url(url)\n",
    "        num_jobs = int(response.json()['count'])\n",
    "        return num_jobs\n",
    "\n",
    "    def scrape_data(self):\n",
    "        \"\"\"Scrapes data and outputs to csv file. \"\"\"\n",
    "        print(\"Scraping Google...\")\n",
    "        google = self.get_dataframe(1)\n",
    "        num_jobs = self.get_num_jobs() # get total number of results\n",
    "        num_pages = math.ceil(num_jobs/20)\n",
    "        for i in range(2,num_pages+1):\n",
    "            google = pd.concat([self.get_dataframe(i), google])\n",
    "        google.to_csv(OUTPUT_DIR+'google.csv')\n",
    "        print(\"Done scraping.\")\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Reads in csv, cleans data, and outputs to csv file.\"\"\"\n",
    "        print(\"Extracting Google US Engineering jobs...\")\n",
    "        df_google = pd.read_csv(OUTPUT_DIR+'google.csv')\n",
    "        df_google = df_google[df_google['job_title'].str.lower().str.contains('engineer')] # get only titles with \"engineer\"\n",
    "        df_google = df_google.reset_index()\n",
    "        df_google = df_google.drop(['Unnamed: 0', 'index'], axis=1)\n",
    "        df_google.to_csv(OUTPUT_DIR+'google_US_engineer.csv')\n",
    "\n",
    "    def tabulate_data(self):\n",
    "        \"\"\"Tabulates data by reading in csv file and outputting tabbed csv file.\"\"\"\n",
    "        print(\"Tabulating number of open job titles by city for Google...\")\n",
    "        df_google = pd.read_csv(OUTPUT_DIR+'google_US_engineer.csv')\n",
    "        location_lst = []\n",
    "        for i in range(len(df_google)):\n",
    "            loc_lst = ast.literal_eval(df_google.locations[i])\n",
    "            for loc in loc_lst:\n",
    "                location_lst.append(loc)\n",
    "        loc_tabbed = pd.Series(location_lst).astype('str').value_counts()\n",
    "        df_google_tabbed = pd.DataFrame(loc_tabbed)\n",
    "        df_google_tabbed.rename(columns={0: \"num_job_openings\"}, inplace = True)\n",
    "        df_google_tabbed.to_csv(OUTPUT_DIR + 'google_tabbed.csv')\n",
    "        print(\"Here are the number of open job titles by city:\")\n",
    "        print(df_google_tabbed)\n",
    "\n",
    "\n",
    "class HomeDepot(Company):\n",
    "\n",
    "    def get_url(self, url):\n",
    "        \"\"\"For a given job posting URL, returns the beautiful soup object.\n",
    "        Arguments:\n",
    "        url -- URL to query\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        # Wait for page to load (code from: https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72)\n",
    "        timeout = 5 # wait 5 seconds\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='jobTitle']\")))\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load\")\n",
    "            driver.quit()\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    def get_list_text(self, original_list):\n",
    "        \"\"\"Gets the text from the html object\"\"\"\n",
    "        return [i.get_text() for i in original_list] # uses list comprehensions!\n",
    "\n",
    "    def get_dataframe(self, num):\n",
    "        \"\"\"Takes list of Google jobs for a given page number and inserts it into a dataframe. Returns a dataframe.\n",
    "        Arguments:\n",
    "        num -- page number\n",
    "        \"\"\"\n",
    "        num_str = str(num)\n",
    "        url = 'https://careers.homedepot.com/job-search-results/?keyword=engineer&location=United%20States&country=US&radius=15&pg=' + num_str\n",
    "        soup = self.get_url(url)\n",
    "        jobtitle_lst = self.get_list_text(soup.find_all(class_='jobTitle'))\n",
    "        jobtype_lst = self.get_list_text(soup.find_all(class_='flex_column av_one_sixth', role='cell'))\n",
    "        jobtype_lst = [x for x in jobtype_lst if x!='']\n",
    "        jobcat_lst = self.get_list_text(soup.find_all(class_='flex_column av_one_fourth', role='cell'))\n",
    "        location_lst = self.get_list_text(soup.find_all(class_='flex_column joblist-location av_one_sixth', role='cell'))\n",
    "        df_homedepot = pd.DataFrame()\n",
    "        df_homedepot['job_title'] = jobtitle_lst\n",
    "        df_homedepot['location'] = location_lst\n",
    "        df_homedepot['category'] = jobcat_lst\n",
    "        df_homedepot['type'] = jobtype_lst\n",
    "        return df_homedepot\n",
    "\n",
    "    def get_num_pages(self):\n",
    "        \"\"\"Computes and returns the total number of pages found.\"\"\"\n",
    "        url = 'https://careers.homedepot.com/job-search-results/?keyword=engineer&location=United%20States&country=US&radius=15&pg=1'\n",
    "        soup = self.get_url(url)\n",
    "        num_results = int(soup.find(id = 'live-results-counter').get_text())\n",
    "        total_num_pages = math.ceil(num_results/10)\n",
    "        return total_num_pages\n",
    "\n",
    "    def scrape_data(self):\n",
    "        \"\"\"Scrapes data and outputs to csv file. \"\"\"\n",
    "        print(\"Scraping Home Depot...\")\n",
    "        homedepot = self.get_dataframe(1)\n",
    "        num_pages = self.get_num_pages()\n",
    "        for i in range(2,num_pages+1):\n",
    "            homedepot = pd.concat([self.get_dataframe(i), homedepot])\n",
    "        homedepot.to_csv(OUTPUT_DIR+'homedepot.csv')\n",
    "        print(\"Done scraping.\")\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Reads in csv, cleans data, and outputs to csv file.\"\"\"\n",
    "        print(\"Extracting Home Depot US Engineering jobs...\")\n",
    "        df_homedepot = pd.read_csv(OUTPUT_DIR+'homedepot.csv')\n",
    "        df_homedepot = df_homedepot[df_homedepot['job_title'].str.lower().str.contains('engineer')] # get only titles with \"engineer\"\n",
    "        df_homedepot = df_homedepot.reset_index()\n",
    "        df_homedepot = df_homedepot.drop(['Unnamed: 0', 'index'], axis=1)\n",
    "        df_homedepot.to_csv(OUTPUT_DIR+'homedepot_US_engineer.csv')\n",
    "\n",
    "    def tabulate_data(self):\n",
    "        \"\"\"Tabulates data by reading in csv file and outputting tabbed csv file.\"\"\"\n",
    "        print(\"Tabulating number of open job titles by city for Home Depot...\")\n",
    "        df_homedepot = pd.read_csv(OUTPUT_DIR+'homedepot_US_engineer.csv')\n",
    "        loc_tabbed = df_homedepot.location.value_counts()\n",
    "        df_homedepot_tabbed = pd.DataFrame(loc_tabbed)\n",
    "        df_homedepot_tabbed.rename(columns={0: \"num_job_openings\"}, inplace = True)\n",
    "        df_homedepot_tabbed.to_csv(OUTPUT_DIR+'homedepot_tabbed.csv')\n",
    "        print(\"Here are the number of open job titles by city:\")\n",
    "        print(df_homedepot_tabbed)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    microsoft = Microsoft().query()\n",
    "    jpmorgan = Jpmorgan().query()\n",
    "    google = Google().query()\n",
    "    homedepot = HomeDepot().query()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
